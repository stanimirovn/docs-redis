---
title: Monitoring Redis for PCF
owner: London Services
---

<strong><%= modified_date %></strong>

The [Loggregator Firehose](https://docs.pivotal.io/pivotalcf/loggregator/architecture.html#firehose) exposes Redis metrics. You can use third-party monitoring tools to consume Redis metrics to monitor Redis performance and health.

As an example of how to display KPIs and metrics, see the [CF Redis example dashboard](https://github.com/pivotal-cf/metrics-datadog-dashboard), which uses [Datadog](https://www.datadoghq.com). Pivotal does not endorse or provide support for any third-party solution.

## <a id="polling"></a> Metrics Polling Interval
The metrics polling interval defaults to 30 seconds. This can be changed by navigating to the Metrics configuration page and entering a new value in **Metrics polling interval (min: 10)**.

![Metrics Polling Interval](metrics_polling.png)

Metrics are emitted in the following format:

<pre class=terminal>
origin:"p-redis" eventType:ValueMetric timestamp:1480084323333475533 deployment:"cf-redis" job:"cf-redis-broker" index:"{redacted}" ip:"10.0.1.49" valueMetric:&#60;name:"/p-redis/service-broker/dedicated_vm_plan/available_instances" value:4 unit:"" >
</pre>

## <a id="kpi"></a>Key Performance Indicators

Key Performance Indicators (KPIs) for Redis for PCF are metrics that operators find most useful for monitoring their Redis service to ensure smooth operation. KPIs are high-signal-value metrics that can indicate emerging issues. KPIs can be raw component metrics or _derived_ metrics generated by applying formulas to raw metrics.

Pivotal provides the following KPIs as general alerting and response guidance for typical Redis for PCF installations.
Pivotal recommends that operators continue to fine-tune the alert measures to their installation by observing historical trends.
Pivotal also recommends that operators expand beyond this guidance and create new, installation-specific monitoring
metrics, thresholds, and alerts based on learning from their own installations.

For a list of all other Redis metrics, see [Other Redis Metrics](#other-metrics).

### <a id="Redis-KPIs"></a> Redis for PCF Service KPIs

#### <a id="total-od-instances"></a> Total Instances For On-Demand Service

<table>
   <tr><th colspan="2" style="text-align: center;"><br> total_instances <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Total instances provisioned by application developers across all On-Demand Services and for a specific On-Demand plan
      <br><br>
      <strong>Use</strong>: Track instance use by app developers.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count<br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Daily</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: N/A  <br>
      <strong>Red critical</strong>: N/A </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      N/A
      </td>
   </tr>
</table>

#### <a id="quota-remaining-od-instances"></a> Quota Remaining For On-Demand Service

<table>
   <tr><th colspan="2" style="text-align: center;"><br> total_instances <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
       <td>Number of available instances across all On-Demand Services and for a specific On-Demand plan.
      <br><br>
      <strong>Use</strong>: Track remaining resources available for app developers.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count<br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Daily</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: 3  <br>
      <strong>Red critical</strong>: 0 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      Increase quota allowed for the specific plan or across all on-demand services.
      </td>
   </tr>
</table>

#### <a id="quota-remaining-dedicated-instances"></a> Quota Remaining For Shared-VM and Dedicated-VM Service

<p class="note"><strong>Note</strong>: As of Redis for PCF v1.11, 
    the on-demand service is at feature parity with the dedicated-VM service.
    The dedicated-VM service plan will be deprecated. 
    Pivotal recommends using the on-demand service plan.</p>

<table>
   <tr><th colspan="2" style="text-align: center;"><br> /p-redis/service-broker/dedicated_vm_plan/available_instances <br>/p-redis/service-broker/shared_vm_plan/available_instances <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
       <td>Number of available instances for the Dedicated-VM serving.
      <br><br>
      <strong>Use</strong>: Track remaining resources available for app developers.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count<br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Daily</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: 2  <br>
      <strong>Red critical</strong>: 0 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      Increase VMs available for the Dedicated-VM service.
      </td>
   </tr>
</table>

### <a id="Redis-KPIs"></a> Redis KPIs

#### <a id="persistent-disk-percent"></a> Percent of Persistent Disk Used

<table>
   <tr><th colspan="2" style="text-align: center;"><br> disk.persistent.percent <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Percentage of persistent disk being used on a VM. The persistent disk is specified as an IaaS-specific disk type with a size. For example, <code>pd-standard</code> on GCP, or <code>st1</code> on AWS, with disk size 5GB. This is a metric relevant to the health of the VM. A percentage of disk usage approaching 100 will cause the VM disk to become unusable as no more files will be allowed to be written.
      <br><br>
      <strong>Use</strong>: Redis is an in-memory data store that uses a persistent disk to backup and restore the dataset in case of upgrades and VM restarts.
      <br><br>
      <strong>Origin</strong>: JMX Bridge or BOSH HM<br>
      <strong>Type</strong>: percent<br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: >75 <br>
      <strong>Red critical</strong>: >90 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      Ensure that the disk is at least 3.5x VM memory. If it is, then contact GSS. If it is not, then then increase disk space.
      </td>
   </tr>
</table>

#### <a id="used-memory-max-memory"></a> Used Memory Percent

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.memory.used_memory / info.memory.maxmemory  <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The ratio of these two metrics returns the percentage of available memory used:<br>
         <ul><li><code>info.memory.used_memory</code> is a metric of the total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc).</li> <li><code>maxmemory</code> is a configuration option for the total memory made available to the Redis instance.</li></ul> 
      <strong>Use</strong>: This is a performance metric that is most critical for Redis instances with a <code>maxmemory-policy</code> of <code>allkeys-lru</code>
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: percentage <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Application-specific based on velocity of data flow. Some options are:
      <br><br>
      <ol>
      <li>Individual data points---Use if key eviction is in place, for example, in cache use cases.</li>
      <li>Average over last 10 minutes---Use if this gives you enough detail.</li>
      <li>Maximum of last 10 minutes</li></ol>

If key eviction is not in place, options 1 or 3 give more useful information to ensure that high usage triggers an alert.</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: 80%
Not applicable for cache usage. When used as a cache, Redis will typically use up to maxmemory and then evict keys to make space for new entries.
<br><br>
A different threshold might be appropriate for specific use cases of no key eviction, to allow for reaction time. Factors to consider:<br><br>
<ol>
<li>Traffic load on application---Higher traffic means that Redis memory will fill up faster.</li>
<li>Average size of data added/ transaction---The more data added to Redis on a single transaction, the faster Redis will fill up its memory.</li></ol>

<strong>Red critical</strong>: 90%. See warning-specific threshold information.
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      No action assuming the maxmemory policy set meets your applications needs. If the maxmemory policy does not persist data as you wish, either coordinate a backup cadence or update your maxmemory policy if using the on-demand Redis service.
      </td>
   </tr>
</table>

#### <a id="connected-clients"></a> Connected Clients

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.clients.connected_clients <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>      
      <td>Number of clients currently connected to the Redis instance.<br><br>
         <strong>Use</strong>: Redis does not close client connections. They remain open until closed explicitly by the client or another script. Once the <code>connected_clients</code> reaches <code>maxclients</code>, Redis stops accepting new connections and begins producing <code>ERR max number of clients reached</code> errors.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: number <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Application-specific. When connected clients reaches max clients, no more clients can connect. This alert should be at the level where it can tell you that your application has scaled to a certain level and may require action.<br>
      <strong>Red critical</strong>: Application-specific. When connected clients reaches max clients, no more clients can connect. This alert should be at the level where it can tell you that your application has scaled to a certain level and may require action.</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
      Increase max clients for your instance if using the on-demand service, or reduce the number of connected clients.
      </td>
   </tr>
</table>

#### <a id="blocked-clients"></a> Blocked Clients

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.clients.blocked_clients <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The number of clients currently blocked waiting for a blocking request they have made to the Redis server. Redis provides two types of primitive commands to retrieve items from lists: standard and blocking. This metric concerns the blocking commands.<br><br>
      
         <strong>Standard Commands</strong><br><br>
      The standard commands (LPOP, RPOP, RPOPLPUSH) immediately return an item from a list. If there are no items available the standard pop commands return nil.<br><br>

      <strong>Blocking Commands</strong><br><br>
      The blocking commands (BLPOP, BRPOP, BRPOPLPUSH) wait for an empty list to become non-empty.
      The client connection is blocked until an item is added to the lists it is watching. Only the client that made the blocking request is blocked, and the Redis server continues to serve other clients. 
      <br><br>The blocking commands each take a <code>timeout</code> argument that is the time in seconds the server waits for a list before returning nil. A blocking command with timeout <code>0</code> waits forever. Multiple clients may be blocked waiting for the same list. For details of the blocking commands, see: <a href="https://redis.io/commands/blpop">https://redis.io/commands/blpop</a>.
      <br><br>
      <strong>Use</strong>: Blocking commands can be useful to avoid clients regularly polling the server for new data. This metric tells you how many clients are currently blocked due to a blocking command.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: number <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Application-specific. Change from baseline may be more significant than actual value.</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: The expected range of the <code>blocked_clients</code> metric depends on what Redis is being used for:
<ul>
<li>Many uses will have no need for blocking commands and should expect <code>blocked_clients</code> to always be zero.</li>
<li> If blocking commands are being used to force a recipient client to wait for a required input, a raised <code>blocked_clients</code> might suggest a problem with the source clients.</li>
<li> <code>blocked_clients</code> might be expected to be high in situations where Redis is being used for infrequent messaging.</li>
</ul>
If <code>blocked_clients</code> is expected to be non-zero, warnings could be based on change from baseline. A sudden rise in <code>blocked_clients</code> could be caused by source clients failing to provide data required by blocked clients.<br><br>
      <strong>Red critical</strong>: There is no <code>blocked_clients</code> threshold critical to the function of Redis.

However a problem that is causing <code>blocked_clients</code> to rise might often cause a rise in <code>connected_clients</code>.
<code>connected_clients</code> does have a hard upper limit and should be used to trigger alerts.</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        Analysis could include:<br><br>
        <ul>
<li>Checking the <code>connected_clients</code> metric. <code>blocked_clients</code> would often rise in concert with <code>connected_clients</code>.</li>
<li>Establishing whether the rise in <code>blocked_clients</code> is accompanied by an overall increase in applications connecting to Redis, or by an asymmetry in clients providing and receiving data with blocking commands</li>
<li>Considering whether a change in <code>blocked_clients</code> is most likely caused by oversupply of blocking requests or undersupply of data</li>
<li>Considering whether a change in network latency is delaying the data from source clients</li></ul>

In general, a rise or change in <code>blocked_clients</code> is more likely to suggest a problem in the network or infrastructure, or in the function of client applications, rather than a problem with the Redis service.
      </td>
   </tr>
</table>

#### <a id="memory-fragmentation-ratio"></a> Memory Fragmentation Ratio

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.memory.mem_fragmentation_ratio <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Ratio of the amount of memory allocated to Redis by the OS to the amount of memory that Redis is using
      <br><br>
      <strong>Use</strong>: A memory fragmentation less than one shows that the memory used by Redis is higher than the OS available memory. In other packagings of redis, large values reflect memory fragmentation. For Redis for PCF, the instances only run Redis meaning that no other processes will be affected by a high fragmentation ratio (e.g., 10 or 11).
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: ratio <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: < 1. Less than 1 indicates that the memory used by Redis is higher than the OS available memory which can lead to performance degradations. <br>
      <strong>Red critical</strong>: Same as warning threshold.</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        Restart the Redis server to normalize fragmentation ratio.
      </td>
   </tr>
</table>

#### <a id="instantaneous-ops-per-sec"></a> Instantaneous Operations Per Second

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.stats.instantaneous_ops_per_sec <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The number of commands processed per second by the Redis server. The <code>instantaneous_ops_per_sec</code> is calculated as the mean of the recent samples taken by the server. The number of recent samples is hardcoded as 16 in the implementation of Redis.
      <br><br>
      <strong>Use</strong>: The higher the commands processed per second, the better the performance of Redis. This is because Redis is single threaded and the commands are processed in sequence. A higher throughput would thus mean faster response per request which is a direct indicator of higher performance. A drop in the number of commands processed per second as compared to historical norms could be a sign of either low command volume or slow commands blocking the system. Low command volume could be normal, or it could be indicative of problems upstream.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Every 30 seconds</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: A drop in the count compared to historical norms could be a sign of either low command volume or slow commands blocking the system. Low command volume could be normal, or it could be indicative of problems upstream. Slow commands could be due to a latency issue, a large number of clients being connected to the same instance, memory being swapped out, etc. Thus, the count is possibly a symptom of compromised Redis performance. However, this is not the case when low command volume is expected.<br><br>
      <strong>Red critical</strong>: A very low count or a large drop from previous counts may indicate a downturn in performance that should result in an investigation. That is unless the low traffic is expected behavior.</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        A drop in the count may be a symptom of compromised Redis performance. The following are possible responses:
   <br><br>
<ol>
   <li><strong>Identify slow commands using the slowlog:</strong><br>
   Redis logs all the commands that take more than a specified amount of time in slowlog. By default, this time is set to 20ms and the slowlog is allowed a maximum of 120 commands. For the purposes of slowlog, execution time is the time taken by Redis alone and does not account for time spent in I/O. So it would not log slow commands solely due to network latency.
   <br><br>
   Given that typical commands, including network latency, take about 200ms, a 20ms Redis execution time is 100 times slower. This could be indicative of memory management issues wherein Redis pages have been swapped to disk.
   <br><br>
   To see all the commands with slow Redis execution times, type <code>slowlog get</code> in the redis-cli.</li>

   <li><strong>Monitor client connections:</strong><br>
   Because Redis is single threaded, one process services requests from all clients. As the number of clients grows, the percentage of resource time given to each client decreases and each client spends an increasing time waiting for their share of Redis server time.
   <br><br>
   Monitoring the number of clients may be important because there may be applications creating connections that you did not expect or your application may not be efficiently closing unused connections.
   <br><br>
   The connected clients metrics can be used to monitor this. This can also be viewed from the redis-cli using the command "info clients".</li>

   <li><strong>Limit client connections:</strong><br>
   This currently defaults to 1000 but depending on the application, you may wish to limit this further. This can be done be running the command "config set maxclients <value>" in the redis-cli. Connections that exceed the limit will be rejected and closed immediately.</value>
   <br><br>
   Setting maxclients is important to limit the number of unintended client connections and should be set to around 110% to 150% of your expected peak number of connections. In addition, because an error message is returned for failed connection attempts, the maxclient limit warns you that a significant number of unexpected connections are occurring. This helps maintain optimal Redis performance.</li>

   <li><strong>Improve memory management:</strong><br>
   Poor memory can cause increased latency in Redis. If your Redis instance is using more memory than is available, the operating system will swap parts of the redis process from out of phyical memory and onto disk. Swapping will significantly reduce Redis performance since reads from disk are about 5 orders or magnitude slower than reads from physical memory.</li>
</ol>


      </td>
   </tr>
</table>

#### <a id="keyspace-hits-keyspace-misses"></a> Keyspace Hits / Keyspace Misses + Keyspace Hits

<table>
   <tr><th colspan="2" style="text-align: center;"><br> info.stats.keyspace_hits / info.stats.keyspace_misses + info.stats.keyspace_hits <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Hit ratio to determine share of keyspace hits that are successful.
      <br><br>
      <strong>Use</strong>: A memory fragmentation less than one shows that the memory used by Redis is higher than the OS available memory. In other packagings of redis, large values reflect memory fragmentation. For Redis for PCF, the instances only run Redis, therefore, no other processes are affected by a high fragmentation ratio (e.g., 10 or 11).
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: ratio <br>
      <strong>Frequency</strong>: 30s (default), 10s (configurable minimum)<br></td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Application-specific</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Application-specific. In general depending how an application is using the cache, an expected hit ratio value can vary between 60% to 99% . Also, the same hit ratio values can mean different things for different applications. Every time an application gets a cache miss, it will probably go to and fetch the data from a slower resource. This cache miss cost can be different per application. The application developers might be able to provide a threshold that is meaningful for the app and its performance<br><br>
      <strong>Red critical</strong>: Application-specific. See the warning threshold above.
   <tr>
      <th>Recommended response</th>
      <td>
        Application-specific. See the warning threshold above. Work with application developers to understand the performance and cache configuration required for their applications.
      </td>
   </tr>
</table>


## <a id="bosh-health"></a> BOSH Health Monitor Metrics

<%= partial '../../redis/odb/bosh_health_metrics_pcf2' %>

## <a id="other-metrics"></a>Other Redis Metrics
Redis also exposes the following metrics. for more information, see the [Redis documentation](http://redis.io/commands/INFO).

* <code>arch\_bits</code>
* <code>uptime\_in\_seconds</code>
* <code>uptime\_in\_days</code>
* <code>hz</code>
* <code>lru\_clock</code>
* <code>client\_longest\_output\_list</code>
* <code>client\_biggest\_input\_buf</code>
* <code>used\_memory\_rss</code>
* <code>used\_memory\_peak</code>
* <code>used\_memory\_lua</code>
* <code>loading</code>
* <code>rdb\_bgsave\_in\_progress</code>
* <code>rdb\_last\_save\_time</code>
* <code>rdb\_last\_bgsave\_time\_sec</code>
* <code>rdb\_current\_bgsave\_time\_sec</code>
* <code>aof\_rewrite\_in\_progress</code>
* <code>aof\_rewrite\_scheduled</code>
* <code>aof\_last\_rewrite\_time\_sec</code>
* <code>aof\_current\_rewrite\_time\_sec</code>
* <code>total\_connections\_received</code>
* <code>total\_commands\_processed</code>
* <code>instantaneous\_ops\_per\_sec</code>
* <code>total\_net\_input\_bytes</code>
* <code>total\_net\_output\_bytes</code>
* <code>instantaneous\_input\_kbps</code>
* <code>instantaneous\_output\_kbps</code>
* <code>rejected\_connections</code>
* <code>sync\_full</code>
* <code>sync\_partial\_ok</code>
* <code>sync\_partial\_err</code>
* <code>expired\_keys</code>
* <code>evicted\_keys</code>
* <code>keyspace\_hits</code>
* <code>keyspace\_misses</code>
* <code>pubsub\_channels</code>
* <code>pubsub\_patterns</code>
* <code>latest\_fork\_usec</code>
* <code>migrate\_cached\_sockets</code>
* <code>repl\_backlog\_active</code>
* <code>repl\_backlog\_size</code>
* <code>repl\_backlog\_first\_byte\_offset</code>
* <code>repl\_backlog\_histlen</code>
* <code>used\_cpu\_sys</code>
* <code>used\_cpu\_user</code>
* <code>used\_cpu\_sys\_children</code>
* <code>used\_cpu\_user\_children</code>
* <code>rdb\_last\_bgsave\_status</code>
* <code>aof\_last\_bgrewrite\_status</code>
* <code>aof\_last\_write\_status</code>


## <a id="availability"></a> Calculating Redis Service Availability

This section describes a method for calculating and monitoring Redis Service uptime.

There are many methods to detect and calculate service availability. 
Pivotal recommends a method that uses metrics emitted regularly by each service instance. 
The exact precision and configuration of this method depends on the [metrics polling interval](#polling) 
configured for the service.

Pivotal uses Datadog as an example third-party metrics dashboard service. 
Pivotal does not endorse or provide support for any third-party solution.

### Calculating Uptime using Expected Metrics
Every interval, each Redis service instance emits approximately 50 different metrics, listed under
[Key Performance Indicators](#kpi). 
All of these metrics are only emitted if the Redis Server process is running. 
They are emitted at every interval. 
You can detect downtime by counting the number of metrics received and 
comparing it to the number of metrics expected. This method ignores the values of the metrics. 
Redis is considered available if a selected metric is received, regardless of its value.

When metrics are visualized, they are aggregated across time to reduce 
the number of points on a graph. In many visualization services the default method 
for aggregation is to take the average value across a period. 
Unfortunately this does not conserve the number of metrics received in a period.

In Datadog a graph displays approximately 200 points. 
If the graph is displaying uptime across a day, 
the expected number of metrics is 2880 - one every 30 seconds. 
By default, these are divided into 200 intervals, and the graph shows the 
average value of the metric for each interval.
However, you can specify a `rollup` command. 
This instructs Datadog to change how it aggregates data 
by providing the count rather than the average value.
With `rollup(count)` Datadog provides the number of the metrics in each 
interval instead of the average value. In this way, 
the 'rollup(count)' command transforms the data by ignoring 
the actual value of each metric in favour of the number of metrics.

In this example we have chosen the <code>pubsub\_channels</code> metric because 
it is obvious that the value of this metric has no bearing on Redis uptime.

Graphs in Datadog can be described in JSON. The following JSON describes a graph 
of daily percentage uptime:

 <pre class="terminal">
{
  "viz": "timeseries",
  "status": "done",
  "requests": [
    {
      "q": "avg:datadog.nozzle.p_redis.p_redis_info_stats_pubsub_channels{deployment:cf-redis} by {ip}.rollup(count, 86400)/2880*100",
      "type": "line",
      "style": {
        "palette": "dog_classic",
        "type": "solid",
        "width": "normal"
      },
      "conditional_formats": [],
      "aggregator": "avg"
    }
  ],
  "autoscale": true,
  "yaxis": {
    "includeZero": false,
    "max": "100"
  }
}
</pre>

![Datadog graph of redis service uptime per day](redis-service-uptime-per-day-datadog.png)

- `cf-redis` is the name of the PCF Redis deployment.
- `datadog.nozzle.p_redis.p_redis_info_stats_pubsub_channels` is the name of the metric.
- `avg: ... {ip}` requests the average of the aggregated metric for each IP address. 
Because each redis instance has a different IP, this has the effect of drawing a line for each service instance.
- There was a long period of downtime on January 7th/8th.
- Uptime on the last day is not accurate because the metrics count from the day-so-far is averaged across the expected metrics for a full day. This is because Datadog aggregates based on true days (0000-2400) rather than the last 24 hours.


One limitation of this calculation is that it assumes it is always possible to divide the received metrics into periods 86400 seconds long. This means that a very long period of downtime will be rounded down to 86400 seconds. We can avoid this error by dividing the count in each period by the length of the period. The following JSON demonstrates this. The interval between aggregated metrics is given by the Datadog `dt` command. The graph should be identical unless there is a period of downtime longer than the rollup aggregation interval.
```
{
  "viz": "timeseries",
  "status": "done",
  "requests": [
    {
      "q": "((avg:datadog.nozzle.p_redis.p_redis_info_stats_pubsub_channels{deployment:cf-redis} by {ip}.rollup(count, 86400)*30)/dt(avg:datadog.nozzle.p_redis.p_redis_info_stats_pubsub_channels{deployment:cf-redis} by {ip}.rollup(count, 86400)))*100",
      "type": "line",
      "style": {
        "palette": "dog_classic",
        "type": "solid",
        "width": "normal"
      },
      "conditional_formats": [],
      "aggregator": "avg"
    }
  ],
  "autoscale": true,
  "yaxis": {
    "max": "100",
    "includeZero": false
  }
}
```

This graph is available on the [CF Redis example dashboard](https://github.com/pivotal-cf/metrics-datadog-dashboard).

#### Limitations of this method
- We assume that every metric emitted by a healthy PCF Redis service instance will be recieved by the metrics aggregation service.
- We assume that if Redis is emitting metrics from the INFO command, the process is also responding to other commands such as GET and SET.
- Downtime of less than the metrics polling interval cannot be detected. Periods of downtime are rounded up to the nearest interval. Overall uptime may therefore be slightly underestimated.
- Downtime in the last period on the graph is not accurate.


### Alternative methods for calculating availability
#### Redis Uptime in Seconds metric
- Redis provides a metric <code>uptime\_in\_seconds</code>.
- The value is the number of seconds the Redis process has been running for since the last restart. A graph of this value has a distinctive saw-tooth pattern with a fall at each period of downtime. It would be possible to compute very accurate Redis uptime using this metric. Unfortunately this is beyond the scope of popular metrics aggregation services.

#### App Polling
- An independant App could be written that from time to time makes a connection to a Redis service instance and tests whether the service is up.
- This might provide more specific information about Redis availability, such as whether a specific command is available.
- A shorter polling interval could be used to increase sensativity.
- This method relies on the App being up to detect when Redis is down.
